{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "name": "Day-1-Video-3",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python [conda env:tensorflow]",
      "language": "python",
      "name": "conda-env-tensorflow-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WittmannF/udemy-deep-learning/blob/master/section-2/assignment-1-boston-housing-blank.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDdKPdexiUjg",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 1: Predicting Housing Prices from Boston Suburbs\n",
        "\n",
        "In your first assignment, you will train a model with a real dataset. The [Boston Housing Dataset](http://lib.stat.cmu.edu/datasets/boston) contains information collected in 1978 by the U.S Census Service concerning housing in the area of Boston Mass. Your goal is to create a prediction model of the price of a house based on 13 attributes (features). Those features are:\n",
        "\n",
        "- CRIM: This is the per capita crime rate by town\n",
        "- ZN: This is the proportion of residential land zoned for lots larger than\n",
        "25,000 sq.ft.\n",
        "- INDUS: Proportion of non-retail business acres per town\n",
        "- CHAS: Charles River dummy variable (this is equal to 1 if tract bounds river; 0 otherwise)\n",
        "- NOX: Nitric oxides concentration (parts per 10 million)\n",
        "- RM: Average number of rooms per dwelling\n",
        "- AGE: Proportion of owner-occupied units built prior to 1940\n",
        "- DIS: Weighted distances to  ve Boston employment centers\n",
        "- RAD: Index of accessibility to radial highways\n",
        "- TAX: Full-value property-tax rate per 10000 USD\n",
        "- PTRATIO: Pupil-teacher ratio by town\n",
        "- B: Calculated as 1000(Bk - 0.63)^2, where Bk is the proportion of people of African American descent by town\n",
        "- LSTAT: Percentage lower status of the population\n",
        "\n",
        "\n",
        "As target variable, we are going to use last column:\n",
        "\n",
        "- MEDV: Median value of owner-occupied homes \n",
        " \n",
        "\n",
        "Optionally, you can also compare your results with the [Boston Housing Data Science Contest by Kaggle](https://www.kaggle.com/c/boston-housing). Although the contest has already finished, you can still make a late submission and compare your score with the leaderboard. \n",
        "\n",
        "\n",
        "\n",
        "## Getting started\n",
        "\n",
        "You will have to go through the 5 steps that we've seen in the Video 4:\n",
        "1. Exploring the data\n",
        "    - Importing data\n",
        "    - Understanding the data\n",
        "2. Preparing the data\n",
        "    - Scaling\n",
        "    - Transforming\n",
        "    - One-Hot Encoding\n",
        "    - Train/Test Split \n",
        "3. Developing a Base Model\n",
        "4. Checking Predictions\n",
        "5. Improving Results\n",
        "6. (Optional) Compare your results on Kaggle\n",
        "\n",
        "The 4 first steps are partially implemented. You will have to finish them and also implement the step 5.\n",
        "\n",
        "## 1. Exploring the data\n",
        "\n",
        "### Importing the dataset\n",
        "The Boston Housing dataset is available at [keras.io/datasets](https://keras.io/datasets/). Run the next cell in order to download and import the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDaCddq6iUjl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from keras.datasets import boston_housing\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = boston_housing.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zs1FZjFLiUjs",
        "colab_type": "text"
      },
      "source": [
        "Let's explore and understand this dataset.\n",
        "\n",
        "### Understanding the Dataset (Exploratory Data Analysis)\n",
        "\n",
        "The dataset was already splited into training (`X_train` and `y_train`) and testing (`X_test` and `y_test`) subsets. As a review, the training set is used for defining the decision boundary of the neural network (i.e., training the model) while the test set is an independent set used for evaluate how good it is the model with unseen data. Let's get some info from those datasets. We are going to convert the training set into a [Pandas Dataframe](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) in order to get more statistics:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvps8tGOiUjt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# TODO: Number of elements in the training set\n",
        "training_len = len(y_train)\n",
        "\n",
        "# TODO: Number of elements in the test set\n",
        "test_len = len(y_test)\n",
        "\n",
        "# Show the calculated values\n",
        "print(\"There are {} houses in the training set\".format(training_len))\n",
        "print(\"There are {} houses in the test set\".format(test_len))\n",
        "\n",
        "# Convert the training set into a Pandas Dataframe in order to get more statistics:\n",
        "columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'PRICE']\n",
        "df = pd.DataFrame(np.c_[X_train, y_train], columns = columns)\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbrGK-1LiUjy",
        "colab_type": "text"
      },
      "source": [
        "The last column was renamed from **MEDV** to **PRICE** in order to provide more semantics about what we want to predict. Now, let's get some statistics about each feature:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ny9-7rC2iUj0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yb0m5VediUj7",
        "colab_type": "text"
      },
      "source": [
        "And finally let's visualize some of those columns with a scatter matrix using [seaborn](https://seaborn.pydata.org/):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqtVZhx_iUj9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "cols = ['LSTAT', 'INDUS', 'NOX', 'RM', 'PRICE'] \n",
        "sns.pairplot(df[cols], kind='reg', plot_kws={'line_kws':{'color':'orange'}})\n",
        "# NOTE: If there's no output, run again!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZhM97mqiUkD",
        "colab_type": "text"
      },
      "source": [
        "In the previous chart we can check the trends of some features when compared to others. Especially the last row is important, since allow us to check the trends of some features with the PRICE in the y axis. It is also possible to check some histogram plots. \n",
        "> **NOTE:** If the previous line didn't run, it is likely because seaborn is not installed. If you are using hat's your case, just `pip install seaborn` in your terminal. If you are using Google Colab, the Seaborn is already installed by default. \n",
        "\n",
        "## 2. Preparing the data\n",
        "Since the dataset is already splitted into training and testing subsets, it is only missing to scale the dataset.\n",
        "\n",
        "### 2.1 Assignment - Scaling the training data\n",
        "Scale the dataset using standard scaller, in the same way it was presented in the video 4:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ugnb8QwiUkE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"*** Before Scaling ***\")\n",
        "print(\"Mean of each feature in the training set\\n{}\".format(np.round(np.mean(X_train, axis=0))))\n",
        "print(\"Standard deviation of each feature in the training set\\n{}\".format(np.round(np.std(X_train, axis=0))))\n",
        "\n",
        "# Function to standardize the training data\n",
        "def standardize(X, X_mean, X_std):\n",
        "    return (X-X_mean)/X_std\n",
        "\n",
        "# Get mean of each column in the training set\n",
        "X_mean = X_train.mean(axis=0)\n",
        "X_std = X_train.std(axis=0)\n",
        "\n",
        "# Apply standardization to the features in the training and test sets\n",
        "X_train = standardize(X_train, X_mean, X_std)\n",
        "X_test = standardize(X_test, X_mean, X_std)\n",
        "\n",
        "print(\"\\n*** After Scaling ***\")\n",
        "print(\"Mean of each feature in the training set\\n{}\".format(np.round(np.mean(X_train, axis=0))))\n",
        "print(\"Standard deviation of each feature in the training set\\n{}\".format(np.round(np.std(X_train, axis=0))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRCKOKbBiUkH",
        "colab_type": "text"
      },
      "source": [
        "## 3. Developing a Base Model\n",
        "For the loss function, let's use the same that is required in the Kaggle contest, the Root Mean Squared Error (RMSE):\n",
        "\n",
        "$$RMSE = \\sqrt{\\sum_i^n{(y_i - \\hat{y}_i)^2}}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RMz26HuiUkH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def root_mean_squared_error(y_true, y_pred):\n",
        "        return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cESn9Ol2iUkK",
        "colab_type": "text"
      },
      "source": [
        "Now let's run the first and simplest version of our model, in the same way it was done in Video 4:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sv5MuZbuiUkM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 0. Import keras dependencies here\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "# 1. Define your base model here\n",
        "model = Sequential([\n",
        "        Dense(units=1, input_shape=(13,))\n",
        "    ])\n",
        "\n",
        "# 2. Set your optimizer and loss function here\n",
        "opt = SGD()\n",
        "model.compile(optimizer=opt,\n",
        "             loss=[root_mean_squared_error])\n",
        "\n",
        "\n",
        "# 3. Train your model\n",
        "model.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eipuq9gXiUkf",
        "colab_type": "text"
      },
      "source": [
        "When compared to the Video 4, the input shape was increased to 13 since there are 13 features. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhBdWp5_iUkh",
        "colab_type": "text"
      },
      "source": [
        "## 4. Checking Predictions\n",
        "Let's check some numerical and visual results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssCnLu4TiUkj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "def check_predictions(model, X, y, cols):\n",
        "    print(\"\\n**** Evaluating the Test set ****\")\n",
        "    loss_test = model.evaluate(X, y)\n",
        "    print(\"Test Loss: {:.3f}\".format(loss_test))\n",
        "    \n",
        "    for column in cols:\n",
        "        y_pred = model.predict(X)\n",
        "        idx = columns.index(column)\n",
        "        plt.scatter(X[:, idx], y, c='b', alpha=0.5, label=\"True Data\")\n",
        "        plt.scatter(X[:, idx], y_pred, c='orange', alpha=0.5, label=\"Predictions\")\n",
        "        plt.xlabel(column)\n",
        "        plt.ylabel(\"Price\")\n",
        "        plt.legend(loc=0)\n",
        "        plt.show()\n",
        "    \n",
        "check_predictions(model, X_test, y_test, ['RM', 'LSTAT'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXg_f7haiUko",
        "colab_type": "text"
      },
      "source": [
        "We can check both numerically and visually that the results are very bad. Ideally we want a R2 Score close to 1. A value of -7 indicates that the model is very bad. We an also confirm this when comparing the true data and predictions in two features. Your job is going to be to improve those results:\n",
        "\n",
        "## 5. Improving Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxf8XAiYiUkp",
        "colab_type": "text"
      },
      "source": [
        "Now it is your turn! Try yourself some techniques for improving the previous results. First of all, let's learn how to add hidden layers and activation functions in a model. \n",
        "\n",
        "### Adding Hidden Layers and Activation Functions\n",
        "Here are some examples of how some illustrated architectures from [Playground Tensorflow](https://playground.tensorflow.org/) would be programmed in keras:\n",
        "\n",
        "#### Example: One input layer, 4 Hidden layers (6, 5, 5 and 4 units with ReLu activations) and One Output Layer (sigmoid activation):\n",
        "\n",
        "- **Illustration:**\n",
        "![screen shot 2019-02-16 at 14 31 43](https://user-images.githubusercontent.com/5733246/52902488-7967b180-31f8-11e9-84a2-fcb4ffd24159.png)\n",
        "\n",
        "- **Model in Keras:**\n",
        "\n",
        "```\n",
        "model = Sequential([\n",
        "    Dense(units=6, input_shape=(2,), activation='relu')),\n",
        "    Dense(units=5, activation='relu')),\n",
        "    Dense(units=5, activation='relu')),\n",
        "    Dense(units=4, activation='relu')),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "```\n",
        "\n",
        "### Improving the Base Model\n",
        "Here are some ideas:\n",
        "\n",
        "- Increase the number of hidden layers and add activation functions:\n",
        "    - As a [rule of thumb](https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw), you can start trying one hidden layer with the number of neurons in that layer as the mean of the neurons in the input plus output layers. \n",
        "    - Here's an example **two hidden layers** with 13 neurons in the first hidden layer and 7 units in the second hidden layer. ReLu activation function functions in the two hidden layers. One neuron in the output layer with no activation function (since it is a regression problem):\n",
        "    ```\n",
        "    model = Sequential([\n",
        "        Dense(units=13, input_shape=(13,), activation='relu'),\n",
        "        Dense(units=7, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    ```\n",
        "    Use it as your starting point! \n",
        "- Increase the number of epochs\n",
        "- Tune the learning rate\n",
        "- Change the optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXJGxF-miUkq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 0. TODO: Import keras dependencies here\n",
        "from keras.models import ...\n",
        "from keras.layers import ...\n",
        "from keras.optimizers import ...\n",
        "\n",
        "# 1. TODO: Define your base model here\n",
        "model = Sequential([\n",
        "        ...\n",
        "    ])\n",
        "\n",
        "\n",
        "# 2. TODO: Set your optimizer and loss function here\n",
        "opt = ...(lr=...)\n",
        "\n",
        "model.compile(optimizer=opt,\n",
        "             loss=[root_mean_squared_error])\n",
        "\n",
        "# 3. Train your model\n",
        "model.fit(X_train, y_train, epochs=...)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qk7yhAoCiUkt",
        "colab_type": "text"
      },
      "source": [
        "Now let's check the prediction in the test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVEDkdiliUku",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "check_predictions(model, X_test, y_test, ['RM', 'LSTAT'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdxCR_EyiUk3",
        "colab_type": "text"
      },
      "source": [
        "## 6. (Optional) Submit to Kaggle\n",
        "Although the contest is already finished, you can try submitting as a late submission on Kaggle and compare your score with other competitors in the [leaderboard](https://www.kaggle.com/c/boston-housing/leaderboard). You just have to click in the button \"Late Submission\" in the [contest page](https://www.kaggle.com/c/boston-housing) and then upload a CSV file in the submission format of the contest (ID and predictions). \n",
        "\n",
        "In order to try yourself, make sure to first the download the test.csv file in [kaggle.com/c/boston-housing/data](https://www.kaggle.com/c/boston-housing/data) and add it in the folder of your project. If you are using Colab, you might have to mount Google Drive first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLL-oU_eiUk5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "USING_COLAB = False\n",
        "\n",
        "if USING_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    # Update here with the folder of the files of your course\n",
        "    !cd '/content/drive/My Drive/course/day-1'\n",
        "    \n",
        "\n",
        "TEST_PATH = 'kaggle_boston_housing_test.csv'\n",
        "\n",
        "# Load the test.csv file\n",
        "testdf = pd.read_csv(TEST_PATH)\n",
        "testdf.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-F5ePifBiUk_",
        "colab_type": "text"
      },
      "source": [
        "Next, run the following cell in order to create a csv submission file based on your model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rlgtBMxiUk_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the column with the IDs\n",
        "ids = testdf['ID'].values\n",
        "\n",
        "# Apply Standard transform in the test set\n",
        "X_test_kaggle = testdf.drop('ID', axis=1)\n",
        "X_test_kaggle_std = sc.fit_transform(X_test_kaggle)\n",
        "\n",
        "# Get predictions\n",
        "y_pred_kaggle = model.predict(X_test_kaggle_std)\n",
        "\n",
        "# Convert to a dataframe in the submission format\n",
        "submit = pd.DataFrame(np.c_[ids, y_pred_kaggle], columns=['ID', 'medv'])\n",
        "submit['ID'] = submit['ID'].astype(int)\n",
        "\n",
        "# Convert to a CSV file\n",
        "submit.to_csv('my_submission_improved_model.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jjm8X-bLiUlE",
        "colab_type": "text"
      },
      "source": [
        "Below is my best score. You can compare your best score using the [leaderboard](https://www.kaggle.com/c/boston-housing/leaderboard):\n",
        "![screen shot 2019-02-15 at 18 14 04](https://user-images.githubusercontent.com/5733246/52882019-824f7900-314d-11e9-97d7-17e48e4a1770.png)\n"
      ]
    }
  ]
}
