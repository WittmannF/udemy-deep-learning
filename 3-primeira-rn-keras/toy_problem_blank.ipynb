{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "name": "toy-problem-blank.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python [conda env:tensorflow]",
      "language": "python",
      "name": "conda-env-tensorflow-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WittmannF/udemy-deep-learning/blob/master/section-2/toy_problem_blank.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "j7a4139FPMIc"
      },
      "source": [
        "# Coding a Toy Neural Network Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PWEfDAEePLJW"
      },
      "source": [
        "We are going to use the very same example that we've seen in the previous video with the house price based on it's area:\n",
        "\n",
        "\n",
        "![input-example](https://user-images.githubusercontent.com/5733246/52136634-a2e8e080-262f-11e9-8f7a-61d79831d83d.png)\n",
        "\n",
        "\n",
        "Usually when working with Machine Learning or Deep Learning problems, you will have to follow those five steps:\n",
        "1. Exploring the data\n",
        "    - Importing data\n",
        "    - Understanding the data\n",
        "2. Preparing the data\n",
        "    - Scaling\n",
        "    - Transforming\n",
        "    - One-Hot Encoding\n",
        "    - Train/Test Split \n",
        "3. Developing a Base Model\n",
        "4. Checking Predictions\n",
        "5. Improving Results\n",
        "\n",
        "Let's check some of them here! \n",
        "\n",
        "\n",
        "## 1. Import the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOIDW9boibxW",
        "colab_type": "text"
      },
      "source": [
        "Let's create a toy dataset with only 20 areas and 20 prices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CDaYfd4yD2Lj",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IpettqxCD2L8"
      },
      "source": [
        "## 2. Preparing the Data\n",
        "\n",
        "### Scaling Numerical Features\n",
        "Optimizers usually work better when the input data ranges from either -1 to 1 or 0 to 1. This helps the error surface to approach faster to its global minima. For better results, the [Standardization](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) is highly suggested for having a data with zero mean and a standard deviation of 1:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSO0XZn8ibxu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_0CxDqZbD2MQ"
      },
      "source": [
        "### Splitting data into Training and Testing sets\n",
        "\n",
        "Besides scaling the data, it is quite important to split the dataset into training and testing subsets. The training set is going to be used for defining the model (or the decision boundary) and the test set is going to be used for evaluating its performance on unseen data. If we don't use a test set, there's a risk of **overfitting** which is illustrated in the following image:\n",
        "\n",
        "![underfit](https://user-images.githubusercontent.com/5733246/52140129-23600f00-2639-11e9-8c03-308823791377.png)\n",
        "\n",
        "The train/test split can be performed using train_test_split from sklearn:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UmBmrkd4D2MR",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KPiwW_lsQXV8"
      },
      "source": [
        "## 3. Developing a Base Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ujJ0SqAOD2Mh"
      },
      "source": [
        "Let's now use Keras in order to build our first model. First of all, [why keras instead of Tensorflow?](https://colab.research.google.com/drive/14JiUzHH2jaFixSOOuWPwKTfj2fXuwD8Q).\n",
        "\n",
        "When defining a model, there are three major components that you have to be aware:\n",
        "1. Model's architecture: How are layers stacked on each other? Which layers are going to be used?\n",
        "    - Documentation of Layers: https://keras.io/layers/core/\n",
        "    - Guide sequential models: https://keras.io/getting-started/sequential-model-guide/\n",
        "    \n",
        "2. Optimizers and loss function.\n",
        "    - Documentation of optimizers: https://keras.io/optimizers/\n",
        "    - Documentation of types of loss functions: https://keras.io/losses/\n",
        "    - Compile method: https://keras.io/models/sequential/#compile\n",
        "\n",
        "3. Training the model\n",
        "    - Fit method: https://keras.io/models/sequential/#fit\n",
        "\n",
        "\n",
        "Based on the previous documentations, let's define the base model as a single neuron with only one weight and one bias in the following way:\n",
        "![](https://user-images.githubusercontent.com/5733246/52482541-ad0f5f80-2b98-11e9-927c-a37ead68bf90.png)\n",
        "\n",
        "[This reference](https://stackoverflow.com/questions/44747343/keras-input-explanation-input-shape-units-batch-size-dim-etc) can be helpful for defining a model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J-suaOvTD2Mj",
        "colab": {}
      },
      "source": [
        "# 0. Import keras dependencies \n",
        "# TODO: Import the sequential model - https://keras.io/getting-started/sequential-model-guide/\n",
        "\n",
        "# TODO: Import the dense layer - https://keras.io/layers/core/\n",
        "\n",
        "# TODO: Import the SGD optimizer - https://keras.io/optimizers/\n",
        "\n",
        "# 1. Define your base model here\n",
        "# TODO: Assign Sequential to model and create a list with just one Dense layer with one unit and one input\n",
        "model = None\n",
        "\n",
        "# 2. Set your optimizer and loss function here\n",
        "# TODO: Initialize the Stochastic Gradient Descent optimizer\n",
        "\n",
        "# TODO: Use the model.compile method with the inputs 'optimizer' and 'loss'\n",
        "model.compile(...)\n",
        "\n",
        "# 3. Train your model\n",
        "# TODO: Use the model.fit method with the the training data as input\n",
        "model.fit(...)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IBamRQyDD2Mu"
      },
      "source": [
        "## 4. Checking Predictions\n",
        "Now let's check how well our base prediction is performing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Yv2FyIkjD2Mv",
        "colab": {}
      },
      "source": [
        "def check_predictions(model, X, y):\n",
        "    y_pred = model.predict(X)\n",
        "    plt.scatter(X, y, c='b', alpha=0.5, label=\"Data\")\n",
        "    plt.plot(X, y_pred, c='r', label=\"Model\")\n",
        "    plt.legend(loc=0)\n",
        "    plt.show()\n",
        "    \n",
        "check_predictions(model, X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ni6ssLlzD2M5"
      },
      "source": [
        "## 5. Improving Results\n",
        "We can see that the model is not fitting well in the dataset. Let's now improve those results! Here are some basic things that we will try:\n",
        "\n",
        "1. Increase the number of epochs\n",
        "    - Epochs is the number of times the algorithm sees the entire data set. For simplicity, you can think here as the number of iterations of the weight\n",
        "2. Changing the optimizer\n",
        "    - Stochastic gradient descent is a very simple optimizers. There are more robusts optimizers such as Adam\n",
        "3. Changing the learning rate\n",
        "4. Adding more layers \n",
        "\n",
        "\n",
        "### 5.1 Increasing the number of epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9o2dluRgD2M9",
        "colab": {}
      },
      "source": [
        "# 0. Import keras dependencies here\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "# 1. Define your base model here\n",
        "model = Sequential([\n",
        "        Dense(units=1, input_shape=(1,))\n",
        "    ])\n",
        "\n",
        "# 2. Set your optimizer and loss function here\n",
        "opt = SGD()\n",
        "model.compile(optimizer=opt,\n",
        "             loss='mean_squared_error')\n",
        "\n",
        "\n",
        "# 3. Train your model\n",
        "model.fit(X_train, y_train, ...)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kSi1UI9xD2NI",
        "colab": {}
      },
      "source": [
        "check_predictions(model, X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RmP1J08_D2Nz"
      },
      "source": [
        "### 5.2 Checking other optimizers\n",
        "Here's a great repository comparing different optimizers from TensorFlow: https://github.com/Jaewan-Yun/optimizer-visualization\n",
        "![](https://github.com/Jaewan-Yun/optimizer-visualization/raw/master/figures/movie11.gif)\n",
        "\n",
        "![](https://github.com/Jaewan-Yun/optimizer-visualization/raw/master/figures/movie9.gif)\n",
        "\n",
        "\n",
        "Let's now try other optimizers that are available from the documentation: https://keras.io/optimizers/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1Jbl-m28D2N0",
        "colab": {}
      },
      "source": [
        "# 0. Import keras dependencies here\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# 1. Define your base model here\n",
        "model = Sequential([\n",
        "        Dense(units=1, input_shape=(1,))\n",
        "    ])\n",
        "\n",
        "# 2. Set your optimizer loss function here\n",
        "opt = Adam()\n",
        "model.compile(optimizer=opt,\n",
        "             loss='mean_squared_error')\n",
        "\n",
        "\n",
        "# 3. Train your model\n",
        "model.fit(X_train, y_train, epochs=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N2j9oZafD2N4",
        "colab": {}
      },
      "source": [
        "check_predictions(model, X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iUFb9u6yD2OA"
      },
      "source": [
        "### 5.3 Tuning the Learning Rate\n",
        "Finally let's enhance the learning rate. As a reminder, small values requires more iterations while large values make the model to diverge. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E1NTbeGYD2OC",
        "colab": {}
      },
      "source": [
        "# 0. Import keras dependencies here\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# 1. Define your base model here\n",
        "model = Sequential([\n",
        "        Dense(units=1, input_shape=(1,))\n",
        "    ])\n",
        "\n",
        "# 2. Set your optimizer and loss function here\n",
        "opt = Adam(lr=0.1) # Default of adam is 0.001. Check large and small values, use a value slighly lower than a diverging lr\n",
        "model.compile(optimizer=opt,\n",
        "             loss='mean_squared_error')\n",
        "\n",
        "\n",
        "# 3. Train your model\n",
        "model.fit(X_train, y_train, epochs=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "78M6Bf10D2OM",
        "colab": {}
      },
      "source": [
        "check_predictions(model, X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "collapsed": true,
        "id": "untxwrvTD2OR"
      },
      "source": [
        "## Final considerations\n",
        "Finally, we can also try using more layers in the model. However, we are going to discuss this in the next video, after checking the different activation functions that can be used. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MsT_754AD2OS",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
